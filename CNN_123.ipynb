{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-123.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoangp/isbi-datasets/blob/upload_colab/CNN_123.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBJnjEKcbcwA",
        "colab_type": "text"
      },
      "source": [
        "# 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ni39z_jFP-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mounth gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "project_path = 'gdrive/My Drive/cs9517-19t2-project'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTrMqQiiJqPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !rm -rf gdrive/My\\ Drive/cs9517-19t2-project/folds\n",
        "\n",
        "!mkdir -p gdrive/My\\ Drive/cs9517-19t2-project\n",
        "!mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds\n",
        "\n",
        "!mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold0\n",
        "!mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold0/aug\n",
        "!mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold0/unet\n",
        "!mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold0/unet/models\n",
        "!mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold0/unet/results\n",
        "\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold1\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold1/aug\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold1/unet\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold1/unet/models\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold1/unet/results\n",
        "\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold2\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold2/aug\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold2/unet\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold2/unet/models\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold2/unet/results\n",
        "\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold3\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold3/aug\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold3/unet\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold3/unet/models\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold3/unet/results\n",
        "\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold4\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold4/aug\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold4/unet\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold4/unet/models\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold4/unet/results\n",
        "\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold5\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold5/aug\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold5/unet\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold5/unet/models\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold5/unet/results\n",
        "\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold6\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold6/aug\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold6/unet\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold6/unet/models\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold6/unet/results\n",
        "\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold7\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold7/aug\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold7/unet\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold7/unet/models\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold7/unet/results\n",
        "\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold8\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold8/aug\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold8/unet\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold8/unet/models\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold8/unet/results\n",
        "\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold9\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold9/aug\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold9/unet\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold9/unet/models\n",
        "# !mkdir -p gdrive/My\\ Drive/cs9517-19t2-project/folds/fold9/unet/results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85oB6YJsg6Ri",
        "colab_type": "text"
      },
      "source": [
        "# 2. Load Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfrGKkQy--Kk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7634f1f-f41c-493a-8fe7-fc49d3c89fb3"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.io as io\n",
        "from skimage.transform import resize\n",
        "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
        "from keras.layers import *\n",
        "\n",
        "# get data\n",
        "data_train_path = project_path + '/data/images'\n",
        "data_label_path = project_path + '/data/labels_tiff'\n",
        "data_train_filenames = glob.glob(data_train_path + \"/*.jpg\")\n",
        "data_label_filenames = glob.glob(data_label_path + \"/*.tif\")\n",
        "data_train_filenames.sort()\n",
        "data_label_filenames.sort()\n",
        "\n",
        "# setup cross validation folds\n",
        "folds = []\n",
        "for i in range(10):\n",
        "    fold = {}\n",
        "    test_index = [0+i, 10+i, 20+i]\n",
        "    fold['test_index'] = test_index\n",
        "    fold['train_filenames'] = [data_train_filenames[k] for k in range(len(data_train_filenames)) if not k in test_index]\n",
        "    fold['label_filenames'] = [data_label_filenames[k] for k in range(len(data_label_filenames)) if not k in test_index]\n",
        "    fold['test_filenames'] = [data_train_filenames[k] for k in range(len(data_train_filenames)) if k in test_index]\n",
        "    fold['test_label_filenames'] = [data_label_filenames[k] for k in range(len(data_label_filenames)) if k in test_index]\n",
        "    fold['result_names'] = ['result-' + fold['test_filenames'][k].split('/')[-1].split('.')[0] for k in range(len(fold['test_filenames']))]\n",
        "    folds.append(fold)\n",
        "\n",
        "# define model\n",
        "class UNet:\n",
        "    def __init__(self, target_size = None):\n",
        "        self.target_size = target_size\n",
        "        self.model = None\n",
        "        if target_size:\n",
        "            self.model = self._get_unet()\n",
        "        \n",
        "        self.batch_size = None\n",
        "        self.model_save_path = output_path + '/unet/models'\n",
        "        self.result_save_path = output_path + '/unet/results'\n",
        "\n",
        "    def _get_unet(self):\n",
        "        from keras.models import Model\n",
        "        from keras.optimizers import Adam\n",
        "        inputs = Input((self.target_size, self.target_size,1))\n",
        "        conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "        conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
        "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "        conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "        conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "        conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "        conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "        conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "        conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "        drop4 = Dropout(0.5)(conv4)\n",
        "        pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "        conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
        "        conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
        "        drop5 = Dropout(0.5)(conv5)\n",
        "        up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
        "        merge6 = concatenate([drop4,up6],axis = 3)\n",
        "        conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
        "        conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
        "        up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
        "        merge7 = concatenate([conv3,up7],axis = 3)\n",
        "        conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
        "        conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
        "        up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
        "        merge8 = concatenate([conv2,up8],axis = 3)\n",
        "        conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
        "        conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
        "        up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
        "        merge9 =concatenate([conv1,up9],axis = 3)\n",
        "        conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
        "        conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "        conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "        conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
        "        model = Model(inputs = inputs, outputs = conv10)\n",
        "        model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "        return model\n",
        "    \n",
        "    def disp_summary(self):\n",
        "        seld.model.summary()\n",
        "        \n",
        "    def train(self, X_train, y_train, batch_size, epochs):\n",
        "        # Trains the model for a given number of epochs (iterations on a dataset).\n",
        "        # fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, \n",
        "        #     validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, \n",
        "        #     sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1)\n",
        "        # batch_size: Number of samples per gradient update. If unspecified, batch_size will default to 32.\n",
        "        # verbose: Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
        "        # (https://keras.io/models/model/) \n",
        "        \n",
        "        save_name = \"epochs:{epoch:02d}-loss:{loss:.3f}-val_acc:{val_acc:.3f}\"\n",
        "        \n",
        "        save_name += '-unet' + '.shape-' + str(target_size) + \\\n",
        "                               '.train-' + str(len(X_train)) + \\\n",
        "                               '.batch-' + str(batch_size) + \\\n",
        "                               '.epoch-' + str(epochs)\n",
        "        \n",
        "        from keras.callbacks import ModelCheckpoint\n",
        "        model_checkpoint = ModelCheckpoint(self.model_save_path + '/' + save_name + '.hdf5', \n",
        "                                           monitor='loss', verbose=1, save_best_only=True)       \n",
        "        self.model.fit(X_train, y_train, batch_size, epochs, \n",
        "                       validation_split=0.2, shuffle=True, callbacks=[model_checkpoint])\n",
        "        \n",
        "    def predict(self, X_test):\n",
        "        if np.max(X_test) > 1:\n",
        "            print(\"Convert test data to float32 range 0..1\")\n",
        "            X_test = X_test.astype(np.float32) / 255.0\n",
        "        return self.model.predict(X_test, batch_size=self.batch_size, verbose=1)\n",
        "    \n",
        "    def predict_and_save(self, X_test, save_names = None, save_format = 'jpg'):\n",
        "        y_pred = self.predict(X_test)   \n",
        "        print('Saving ' + str(len(y_pred)) + ' images to ' + self.result_save_path)       \n",
        "        for i in range(len(y_pred)):\n",
        "            save_name = save_names[i] if save_names else str(i)\n",
        "            io.imsave(self.result_save_path + \"/\" + save_name + \".\" + save_format, y_pred[i])       \n",
        "        return y_pred\n",
        "        \n",
        "    def load(self, filepath):         \n",
        "        # extract model info from filename\n",
        "        s_path   = filepath.split('/')\n",
        "        s_name   = s_path[-1].split('.')  # ['unet', 'shape-128', 'train-700', 'batch-4', 'epoch-5', 'hdf5']\n",
        "        s_target = s_name[1].split('-')   # ['shape', '128']\n",
        "        s_batch  = s_name[3].split('-')   # ['batch', '4']\n",
        "        self.target_size = int(s_target[1])\n",
        "        self.batch_size  = int(s_batch[1])\n",
        "        \n",
        "        print('loading model ' +  s_path[-1] + ' ...')     \n",
        "        \n",
        "        from keras.models import load_model\n",
        "        self.model = load_model(filepath)   "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEtpjqGXhiBY",
        "colab_type": "text"
      },
      "source": [
        "# 3. One Big Script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7uPMiw1bf9p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0c723a9d-7a4b-46af-aab6-48c21e3e76ee"
      },
      "source": [
        "# PARAMETERS\n",
        "image_size = 512\n",
        "target_size = 128     # smaller target_size for quicker training\n",
        "num_aug_batches = 64  # total augmented images = num_imgs * num_aug_batches\n",
        "train_batch_size = 4\n",
        "train_epochs = 10\n",
        "\n",
        "# FOLD NUMBER\n",
        "fn = 0\n",
        "\n",
        "print('\\n======')\n",
        "print('FOLD',fn)\n",
        "print('======\\n')\n",
        "\n",
        "output_path     = project_path + '/folds/fold' + str(fn)  \n",
        "aug_path        = output_path + '/aug' \n",
        "\n",
        "print('\\n----------------------------------')\n",
        "print('Load and pre-process original data')\n",
        "print('----------------------------------\\n')\n",
        "\n",
        "# get data images and labels filenames\n",
        "train_filenames = folds[fn]['train_filenames'] \n",
        "label_filenames = folds[fn]['label_filenames'] \n",
        "test_filenames = folds[fn]['test_filenames'] \n",
        "test_label_filenames = folds[fn]['test_label_filenames'] \n",
        "\n",
        "# get train data\n",
        "X_org = np.ndarray((len(train_filenames), target_size, target_size, 1), dtype=np.uint8)\n",
        "y_org = np.ndarray((len(train_filenames), target_size, target_size, 1), dtype=np.uint8)\n",
        "for i in range(len(train_filenames)):\n",
        "    arr_t = img_to_array(load_img(train_filenames[i], color_mode = \"grayscale\"))\n",
        "    arr_l = img_to_array(load_img(label_filenames[i], color_mode = \"grayscale\"))\n",
        "    if i % 10 == 0:\n",
        "        print('loading', train_filenames[i].split('/')[-1], 'and', \n",
        "                         label_filenames[i].split('/')[-1], '...')\n",
        "    # resize for faster training\n",
        "    if target_size < image_size:\n",
        "        arr_t = resize(arr_t, (target_size, target_size, 1), \n",
        "                       mode = 'constant', preserve_range = True)\n",
        "        arr_l = resize(arr_l, (target_size, target_size, 1), \n",
        "                       mode = 'constant', preserve_range = True)        \n",
        "    X_org[i] = arr_t\n",
        "    y_org[i] = arr_l\n",
        "\n",
        "# get test data\n",
        "X_test = np.ndarray((len(test_filenames), target_size, target_size, 1), dtype=np.uint8)\n",
        "y_test = np.ndarray((len(test_filenames), target_size, target_size, 1), dtype=np.uint8)\n",
        "for i in range(len(test_filenames)):\n",
        "    arr_t = img_to_array(load_img(test_filenames[i], color_mode = \"grayscale\"))\n",
        "    arr_l = img_to_array(load_img(test_label_filenames[i], color_mode = \"grayscale\"))\n",
        "    # resize for faster training\n",
        "    if target_size < image_size:\n",
        "        arr_t = resize(arr_t, (target_size, target_size, 1), \n",
        "                       mode = 'constant', preserve_range = True)\n",
        "        arr_l = resize(arr_l, (target_size, target_size, 1), \n",
        "                       mode = 'constant', preserve_range = True)        \n",
        "    X_test[i] = arr_t\n",
        "    y_test[i] = arr_l\n",
        "\n",
        "print('X_org : shape =', X_org.shape, ', min =', np.min(X_org), ', max =', np.max(X_org))\n",
        "print('y_org : shape =', y_org.shape, ', min =', np.min(y_org), ', max =', np.max(y_org))\n",
        "print('X_test: shape =', X_test.shape, ', min =', np.min(X_test), ', max =', np.max(X_test))\n",
        "print('y_test: shape =', y_test.shape, ', min =', np.min(y_test), ', max =', np.max(y_test))\n",
        "\n",
        "print('\\n-------------------------')\n",
        "print('Generate augmented images')\n",
        "print('-------------------------\\n')\n",
        "\n",
        "datagen = ImageDataGenerator(rotation_range=10, shear_range=0.1,  zoom_range=0.1,\n",
        "     horizontal_flip=True, vertical_flip=True, fill_mode='constant', cval=0)\n",
        "\n",
        "# merge label and train, one by one augmentation\n",
        "for i in range(len(X_org)):\n",
        "    arr_t = X_org[i]\n",
        "    arr_l = y_org[i]\n",
        "\n",
        "    s = np.shape(arr_t) # either image_size or target_size\n",
        "\n",
        "    # Create data & label arrays for ImageDataGenerator.flow()\n",
        "    arr_tl = np.ndarray(shape=(s[0],s[1],3), dtype=np.uint8) # (size, size, 3)\n",
        "    arr_tl[:,:,0] = arr_t[:,:,0]\n",
        "    arr_tl[:,:,2] = arr_l[:,:,0]\n",
        "    arr_tl = arr_tl.reshape((1,) + arr_tl.shape) # (1, size, size, 3) -> Rank 4\n",
        "\n",
        "    if i % 10 == 0:\n",
        "        print(\"Generate augmented data for data & label array \" + str(i) + \" ...\")\n",
        "\n",
        "    # Takes data & label arrays, generates batches of augmented data.\n",
        "    # flow(x, y=None, batch_size=32, shuffle=True, sample_weight=None, seed=None, \n",
        "    #      save_to_dir=None, save_prefix='', save_format='png', subset=None)\n",
        "    batches = 0\n",
        "    for batch in datagen.flow(arr_tl, batch_size=1, save_to_dir=aug_path, save_prefix=str(i)):\n",
        "        batches += 1\n",
        "        if batches >= num_aug_batches:\n",
        "            break\n",
        "            # we need to break the loop by hand because\n",
        "            # the generator loops indefinitely       \n",
        "\n",
        "aug_filenames = glob.glob(aug_path + \"/*.*\")\n",
        "aug_filenames.sort()\n",
        "\n",
        "print(\"No. of augmented images =\", len(aug_filenames), \"(saved to \" + aug_path + \")\")\n",
        "\n",
        "print('\\n---------------------------------------------------------')\n",
        "print('Create train data from original data and augmented images')\n",
        "print('---------------------------------------------------------\\n')\n",
        "\n",
        "X_train = np.ndarray((len(X_org) + len(aug_filenames), target_size, target_size, 1), dtype=np.float32)\n",
        "y_train = np.ndarray((len(X_org) + len(aug_filenames), target_size, target_size, 1), dtype=np.float32)\n",
        "\n",
        "# Use X_org and y_org as train data\n",
        "print(\"loading X_org/y_org as training data ...\")\n",
        "for i in range(len(X_org)):\n",
        "    X_train[i] = X_org[i]\n",
        "    y_train[i] = y_org[i]\n",
        "\n",
        "# Load images from augmentation folder\n",
        "for i in range(len(aug_filenames)):\n",
        "    if i % 100 == 0:\n",
        "        print(\"loading augmented image \" + str(i) + \" ...\")\n",
        "    arr = img_to_array(load_img(aug_filenames[i]))\n",
        "    X_train[len(X_org) + i] = arr[:,:,:1]\n",
        "    y_train[len(X_org) + i] = arr[:,:,2:]\n",
        "\n",
        "# convert X_train / y_train to 0..1\n",
        "X_train /= 255.0\n",
        "y_train /= 255.0\n",
        "\n",
        "# convert y_train to 0 or 1\n",
        "y_train[y_train > 0.5] = 1\n",
        "y_train[y_train <= 0.5] = 0\n",
        "\n",
        "print('X_train: shape =', X_train.shape, ', min =', np.min(X_train), ', max =', np.max(X_train))\n",
        "print('y_train: shape =', y_train.shape, ', min =', np.min(y_train), ', max =', np.max(y_train))\n",
        "\n",
        "# Load model\n",
        "model = UNet(target_size)\n",
        "\n",
        "print('\\n-----')\n",
        "print('TRAIN')\n",
        "print('-----\\n')\n",
        "\n",
        "model.train(X_train, y_train, batch_size=train_batch_size, epochs=train_epochs)\n",
        "\n",
        "print('\\n--------------------------------')\n",
        "print('Predict and save results to disk')\n",
        "print('--------------------------------\\n')\n",
        "\n",
        "y_pred = model.predict_and_save(X_test, save_names = folds[fn]['result_names'], save_format = 'jpg')\n",
        "y_pred = model.predict_and_save(X_test, save_names = folds[fn]['result_names'], save_format = 'tif')\n",
        "\n",
        "print('\\n-----------------')\n",
        "print('Visualize results')\n",
        "print('-----------------\\n')\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    img_t = array_to_img(X_test[i])\n",
        "    img_p = array_to_img(y_pred[i])\n",
        "    img_l = array_to_img(y_test[i])\n",
        "\n",
        "    plt.figure(i, figsize=(15, 5))\n",
        "    plt.imshow(np.hstack((img_t, img_p, img_l)), cmap='gray')\n",
        "    plt.title(folds[fn]['result_names'][i])\n",
        "    plt.axis('off')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======\n",
            "FOLD 0\n",
            "======\n",
            "\n",
            "\n",
            "----------------------------------\n",
            "Load and pre-process original data\n",
            "----------------------------------\n",
            "\n",
            "loading train-volume01.jpg and train-labels01.tif ...\n",
            "loading train-volume12.jpg and train-labels12.tif ...\n",
            "loading train-volume23.jpg and train-labels23.tif ...\n",
            "X_org : shape = (27, 128, 128, 1) , min = 2 , max = 252\n",
            "y_org : shape = (27, 128, 128, 1) , min = 0 , max = 255\n",
            "X_test: shape = (3, 128, 128, 1) , min = 1 , max = 247\n",
            "y_test: shape = (3, 128, 128, 1) , min = 0 , max = 255\n",
            "\n",
            "-------------------------\n",
            "Generate augmented images\n",
            "-------------------------\n",
            "\n",
            "Generate augmented data for data & label array 0 ...\n",
            "Generate augmented data for data & label array 10 ...\n",
            "Generate augmented data for data & label array 20 ...\n",
            "No. of augmented images = 1722 (saved to gdrive/My Drive/cs9517-19t2-project/folds/fold0/aug)\n",
            "\n",
            "---------------------------------------------------------\n",
            "Create train data from original data and augmented images\n",
            "---------------------------------------------------------\n",
            "\n",
            "loading X_org/y_org as training data ...\n",
            "loading augmented image 0 ...\n",
            "loading augmented image 100 ...\n",
            "loading augmented image 200 ...\n",
            "loading augmented image 300 ...\n",
            "loading augmented image 400 ...\n",
            "loading augmented image 500 ...\n",
            "loading augmented image 600 ...\n",
            "loading augmented image 700 ...\n",
            "loading augmented image 800 ...\n",
            "loading augmented image 900 ...\n",
            "loading augmented image 1000 ...\n",
            "loading augmented image 1100 ...\n",
            "loading augmented image 1200 ...\n",
            "loading augmented image 1300 ...\n",
            "loading augmented image 1400 ...\n",
            "loading augmented image 1500 ...\n",
            "loading augmented image 1600 ...\n",
            "loading augmented image 1700 ...\n",
            "X_train: shape = (1749, 128, 128, 1) , min = 0.0 , max = 0.9882353\n",
            "y_train: shape = (1749, 128, 128, 1) , min = 0.0 , max = 1.0\n",
            "\n",
            "-----\n",
            "TRAIN\n",
            "-----\n",
            "\n",
            "Train on 1399 samples, validate on 350 samples\n",
            "Epoch 1/10\n",
            "1399/1399 [==============================] - 48s 34ms/step - loss: 0.3796 - acc: 0.8414 - val_loss: 0.3491 - val_acc: 0.8763\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.37959, saving model to gdrive/My Drive/cs9517-19t2-project/folds/fold0/unet/models/epochs:01-loss:0.380-val_acc:0.876-unet.shape-128.train-1749.batch-4.epoch-10.hdf5\n",
            "Epoch 2/10\n",
            "1399/1399 [==============================] - 42s 30ms/step - loss: 0.2997 - acc: 0.9026 - val_loss: 0.3298 - val_acc: 0.8886\n",
            "\n",
            "Epoch 00002: loss improved from 0.37959 to 0.29970, saving model to gdrive/My Drive/cs9517-19t2-project/folds/fold0/unet/models/epochs:02-loss:0.300-val_acc:0.889-unet.shape-128.train-1749.batch-4.epoch-10.hdf5\n",
            "Epoch 3/10\n",
            "1399/1399 [==============================] - 42s 30ms/step - loss: 0.2784 - acc: 0.9149 - val_loss: 0.3272 - val_acc: 0.8914\n",
            "\n",
            "Epoch 00003: loss improved from 0.29970 to 0.27841, saving model to gdrive/My Drive/cs9517-19t2-project/folds/fold0/unet/models/epochs:03-loss:0.278-val_acc:0.891-unet.shape-128.train-1749.batch-4.epoch-10.hdf5\n",
            "Epoch 4/10\n",
            "1399/1399 [==============================] - 42s 30ms/step - loss: 0.2635 - acc: 0.9229 - val_loss: 0.3162 - val_acc: 0.8977\n",
            "\n",
            "Epoch 00004: loss improved from 0.27841 to 0.26347, saving model to gdrive/My Drive/cs9517-19t2-project/folds/fold0/unet/models/epochs:04-loss:0.263-val_acc:0.898-unet.shape-128.train-1749.batch-4.epoch-10.hdf5\n",
            "Epoch 5/10\n",
            "1399/1399 [==============================] - 42s 30ms/step - loss: 0.2506 - acc: 0.9298 - val_loss: 0.3068 - val_acc: 0.9023\n",
            "\n",
            "Epoch 00005: loss improved from 0.26347 to 0.25058, saving model to gdrive/My Drive/cs9517-19t2-project/folds/fold0/unet/models/epochs:05-loss:0.251-val_acc:0.902-unet.shape-128.train-1749.batch-4.epoch-10.hdf5\n",
            "Epoch 6/10\n",
            "1399/1399 [==============================] - 42s 30ms/step - loss: 0.2405 - acc: 0.9350 - val_loss: 0.3037 - val_acc: 0.9046\n",
            "\n",
            "Epoch 00006: loss improved from 0.25058 to 0.24049, saving model to gdrive/My Drive/cs9517-19t2-project/folds/fold0/unet/models/epochs:06-loss:0.240-val_acc:0.905-unet.shape-128.train-1749.batch-4.epoch-10.hdf5\n",
            "Epoch 7/10\n",
            "1399/1399 [==============================] - 42s 30ms/step - loss: 0.2314 - acc: 0.9393 - val_loss: 0.3027 - val_acc: 0.9060\n",
            "\n",
            "Epoch 00007: loss improved from 0.24049 to 0.23141, saving model to gdrive/My Drive/cs9517-19t2-project/folds/fold0/unet/models/epochs:07-loss:0.231-val_acc:0.906-unet.shape-128.train-1749.batch-4.epoch-10.hdf5\n",
            "Epoch 8/10\n",
            "1399/1399 [==============================] - 42s 30ms/step - loss: 0.2241 - acc: 0.9424 - val_loss: 0.3021 - val_acc: 0.9071\n",
            "\n",
            "Epoch 00008: loss improved from 0.23141 to 0.22408, saving model to gdrive/My Drive/cs9517-19t2-project/folds/fold0/unet/models/epochs:08-loss:0.224-val_acc:0.907-unet.shape-128.train-1749.batch-4.epoch-10.hdf5\n",
            "Epoch 9/10\n",
            "1399/1399 [==============================] - 42s 30ms/step - loss: 0.2172 - acc: 0.9451 - val_loss: 0.3061 - val_acc: 0.9075\n",
            "\n",
            "Epoch 00009: loss improved from 0.22408 to 0.21720, saving model to gdrive/My Drive/cs9517-19t2-project/folds/fold0/unet/models/epochs:09-loss:0.217-val_acc:0.907-unet.shape-128.train-1749.batch-4.epoch-10.hdf5\n",
            "Epoch 10/10\n",
            "1399/1399 [==============================] - 42s 30ms/step - loss: 0.2105 - acc: 0.9478 - val_loss: 0.3154 - val_acc: 0.9080\n",
            "\n",
            "Epoch 00010: loss improved from 0.21720 to 0.21055, saving model to gdrive/My Drive/cs9517-19t2-project/folds/fold0/unet/models/epochs:10-loss:0.211-val_acc:0.908-unet.shape-128.train-1749.batch-4.epoch-10.hdf5\n",
            "\n",
            "--------------------------------\n",
            "Predict and save results to disk\n",
            "--------------------------------\n",
            "\n",
            "Convert test data to float32 range 0..1\n",
            "3/3 [==============================] - 0s 114ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0730 12:04:17.158282 140294357694336 util.py:61] Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "W0730 12:04:17.166065 140294357694336 util.py:61] Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "W0730 12:04:17.173570 140294357694336 util.py:61] Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving 3 images to gdrive/My Drive/cs9517-19t2-project/folds/fold0/unet/results\n",
            "Convert test data to float32 range 0..1\n",
            "\r3/3 [==============================] - 0s 6ms/step\n",
            "Saving 3 images to gdrive/My Drive/cs9517-19t2-project/folds/fold0/unet/results\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}